# [AAAI 2025] Tokenphormer: Structure-aware Multi-token Graph Transformer

**Tokenphormer** is a novel graph transfomer model designed to address limitations in traditional Graph Neural Networks (GNNs) and Graph Transformers by utilizing multi-token generation. This approach effectively captures both local and global information while retaining graph structural context.

The full version(with Appendix) is online now: [Tokenphormer](https://arxiv.org/abs/2412.15302).

The SGPM Model Link: [pan.quark.cn/s/9d895622e551](https://pan.quark.cn/s/9d895622e551).

## Overview

Graph Neural Networks (GNNs) are commonly used for graph data mining tasks, but they face issues like **over-smoothing** and **over-squashing**, limiting their receptive field. Graph Transformers, on the other hand, offer a global receptive field but suffer from noisy, irrelevant nodes and a loss of structural information.

**Tokenphormer** overcomes these challenges by generating multiple tokens at different levels of granularity, ensuring both local and global information are captured. The model is inspired by fine-grained token-based representation learning from Natural Language Processing (NLP).

## Framework Overview

Below is an illustration of the Tokenphormer framework:

![Framework Overview](framework.png)

## Installation

To set up the environment for Tokenphormer and SGPM, follow these steps:

1. **Clone the repository**:

   ```bash
   git clone https://github.com/your-username/tokenphormer.git
   cd tokenphormer
   ```

2. **Install dependencies**:  
   The project provides environment files to help set up the required dependencies for both Tokenphormer and SGPM.

   - Go to the `Envs` folder and select the appropriate `.yaml` file for your environment.
   - Create a conda environment with the following command:

   ```bash
   conda env create -f Envs/tokenphormer_env.yaml
   ```

   Adjust the environment file name as needed (for example, `sgpm_env.yaml` if you're setting up SGPM). Then activate the environment:

   ```bash
   conda activate tokenphormer_env
   ```

## Running SGPM

1. **Navigate to the SGPM directory**:

   ```bash
   cd SGPM
   ```

2. **Modify the dataset** you want to use by editing `pretraining_args.py`.

3. **Run SGPM**:

   ```bash
   sh run.sh
   ```

   This will pre-train the graph model using SGPM. Once the pre-training is complete, you will need to move the generated files to the Tokenphormer directory.

## Running Tokenphormer

1. **Navigate to the Tokenphormer directory**:

   ```bash
   cd Tokenphormer
   ```

2. **Modify the dataset** you want to use by editing `run.sh`.

3. **Use SGPM-generated tokens**:  
   If you want to run Tokenphormer with SGPM tokens, you need to move the relevant files generated by SGPM into the Tokenphormer directory.

   - **Move SGPM files**:
     - From `SGPM/PRETRAINED_MODELS/dataset/pytorch_model.bin` to `Tokenphormer/GraphPretrainedModel/dataset/pytorch_model.bin`
     - From `SGPM/TOKENIZED_GRAPH_DATESET/dataset/bert_config.json` to `Tokenphormer/GraphPretrainedModel/dataset/config.json`
     - From `SGPM/TOKENIZED_GRAPH_DATESET/dataset/vocab.txt` to `Tokenphormer/GraphPretrainedModel/dataset/vocab.txt`

4. **Run Tokenphormer**:

   ```bash
   sh run.sh
   ```

   This will train Tokenphormer using the SGPM pre-trained model and tokens.

## Citation

If you find Tokenphormer useful for your research, please cite our paper.
```
@inproceedings{zhou2025tokenphormer,
  title={Tokenphormer: Structure-aware Multi-token Graph Transformer for Node Classification},
  author={Zhou, Zijie and Lu, Zhaoqi and Wei, Xuekai and Chen, Rongqin and Zhang, Shenghui and Ip, Pak Lon and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={12},
  pages={13428--13436},
  year={2025}
}
```

## License

This project is licensed under the MIT License.

## Acknowledgements

We would like to thank the authors of the following works for their contributions to the field:

- SGPM: Modified from [Meelfy/pytorch_pretrained_BERT](https://github.com/Meelfy/pytorch_pretrained_BERT).
- Tokenphormer: Modified from [JHL-HUST/NAGphormer](https://github.com/JHL-HUST/NAGphormer).
